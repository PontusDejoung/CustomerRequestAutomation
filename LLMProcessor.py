import ollama
from baseContextPrompt import base_context

class LLMProcessor:
    """
    LLMProcessor is responsible for generating prompts, sending them to a language model, 
    and retrieving responses based on customer questions within a specific context.
    """

    def __init__(self, model_name):
        """
        Initializes the LLMProcessor with a specified model name and base context.

        Parameters:
            model_name (str): The name or identifier of the language model to be used.
        """
        self.model = model_name
        self.base_context = base_context

    def build_prompt(self, question):
        """
        Constructs a prompt to be sent to the language model, using the base context and specific guidelines.
        
        Parameters:
            question (str): The customer's question to be addressed.

        Returns:
            str: The complete prompt, formatted to ensure the language model follows specific instructions
                 and maintains a professional response format.
        """
        prompt = (
            f"{self.base_context}\n\n"
            f"Customer Question: {question}\n\n"
            f"Expected Answer:\n"
            f"- Begin the response with 'Hello, my name is Peter,'.\n"
            f"- Do not add any other greeting or personal comment beyond this phrase.\n"
            f"- Provide a detailed, professional response strictly based on the company's guidelines.\n"
            f"- Answer concisely and clearly, directly addressing the customer's question.\n"
            f"- If applicable, suggest relevant resources or documentation that could assist the customer.\n"
            f"- If the information needed to answer the question is not available in the provided context, respond with 'I'm sorry, but I don't have the information to answer that question.'\n"
            f"- Conclude every relevant response with the exact phrase: 'Best regards, Company Customer Support'.\n"
            f"- After writing 'Best regards, Company Customer Support', end the response immediately. Do not add any further information, suggestions, or closing remarks.\n"
            f"- It is essential to follow these instructions exactly as written, without any deviations.\n"
        )
        return prompt

    def get_response(self, question):
        """
        Generates a prompt based on the customer's question, sends it to the language model, 
        and retrieves the model's response.

        Parameters:
            question (str): The customer's question to be addressed.

        Returns:
            str: The response generated by the language model or an error message if something goes wrong.
        """
        prompt = self.build_prompt(question)
        response = self.send_prompt(prompt)
        return response

    def send_prompt(self, prompt):
        """
        Sends the generated prompt to the language model and retrieves the response.

        Parameters:
            prompt (str): The prompt to send to the language model.

        Returns:
            str: The content of the response message from the language model. If an error occurs,
                 returns an error message indicating the failure.
        
        Raises:
            Exception: Catches any exceptions that occur while communicating with the language model
                       and logs an error message.
        """
        try:
            response = ollama.chat(
                model=self.model,
                messages=[{"role": "user", "content": prompt}]
            )
            return response['message']['content']
        except Exception as e:
            print(f"Error in sending prompt to LLM: {e}")
            return "An error occurred. Please try again later."
